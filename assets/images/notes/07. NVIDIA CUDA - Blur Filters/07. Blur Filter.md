# Linkedin 
ð˜½ð™¡ð™ªð™§ ð™›ð™žð™¡ð™©ð™šð™§ â€” one of the most widely used techniques in image processing â€” applied in visual effects, downsampling, smoothing, motion blur, privacy & obfuscation, and more.
There are different types of blur filters such as box blur, gaussian blur, and radial blur â€” each with its own advantages and trade-offs, and various strategies for GPU implementation like single-pass, hardware downsampling, and 2-tap passes.

There are plenty of articles online covering various blur techniques, so I wonâ€™t go into that here. In this post, Iâ€™ll focus on one of the simplest forms: the box blur. Weâ€™ll implement it in two ways â€” a basic version without shared memory, and an optimized version that leverages the GPUâ€™s shared memory for better performance â€” and compare the results.

ðŸ‘‰ Box Blur
The idea is simple: ð™›ð™¤ð™§ ð™šð™–ð™˜ð™ ð™¥ð™žð™­ð™šð™¡, ð™œð™žð™«ð™šð™£ ð™– ð™§ð™–ð™™ð™žð™ªð™¨, ð™¬ð™š ð™§ð™šð™–ð™™ ð™žð™©ð™¨ ð™£ð™šð™žð™œð™ð™—ð™¤ð™§ð™¨, ð™–ð™«ð™šð™§ð™–ð™œð™š ð™©ð™ð™šð™žð™§ ð™˜ð™¤ð™¡ð™¤ð™§ ð™«ð™–ð™¡ð™ªð™šð™¨, ð™–ð™£ð™™ ð™¬ð™§ð™žð™©ð™š ð™©ð™ð™š ð™§ð™šð™¨ð™ªð™¡ð™©. The larger the radius, the stronger the blur effect.

To apply the blur across the image, we need to launch one thread per pixel.
Remember, CUDA doesnâ€™t simply launch a flat number of threadsâ€”you define a grid of thread blocks and specify how many threads each block contains. 

In this example, we use a 2D ð™—ð™¡ð™¤ð™˜ð™ ð™Žð™žð™¯ð™š = 16Ã—16 to match the imageâ€™s 2D nature.
Then, we compute how many blocks (2D, again) are needed to cover the image:
ð™œð™§ð™žð™™ð™Žð™žð™¯ð™š = ð™žð™¢ð™–ð™œð™šð™Žð™žð™¯ð™š / ð™—ð™¡ð™¤ð™˜ð™ ð™Žð™žð™¯ð™š

Inside the kernel, each thread calculates its corresponding pixel position based on the block ID, block dimensions, and thread ID within the block and apply the blur logic.
Thatâ€™s all you need for a working blur.

ðŸŒŸ But hereâ€™s the catch: each thread reads RADIUS Ã— RADIUS pixels from global memory. This is slow and across thousands of threads, it adds up to significant memory bandwidth usage.
ðŸ”¥ And hereâ€™s the opportunity: neighboring threads within a block read overlapping regions.

Instead of having each thread fetch its own data from global memory, we load a tile of the image (block size + padding) into shared memory. All threads in the block cooperate to read this data, then synchronize, and apply the blur using the shared memory.
This significantly reduces global memory traffic and can result in up to a ~33% performance boost, depending on the hardware.

As weâ€™ll see, thread block size and blur radius directly affect the performance.
Can you guess what would happen if we use 32Ã—32 blocks instead of 16Ã—16?

This is why understanding the GPU architecture matters â€” small changes, like proper use of shared memory, can lead to major performance gains.

The full source code from this post will be available on GitHub.

In the following posts, weâ€™ll revisit the blur filter and explore how to make it even faster using techniques like intra-warp communication and new hardware features such as distributed shared memory.

ðŸ“± Donâ€™t forget that you can also find my posts on Instagram -> https://lnkd.in/dbKdgpE8

#GPU #GPUProgramming #GPUArchitecture #ParallelComputing #CUDA #NVIDIA #AMD #ComputerArchitecture #GPUDemystified

# Instagram
ð˜½ð™¡ð™ªð™§ ð™›ð™žð™¡ð™©ð™šð™§ â€” one of the most widely used techniques in image processing.

There are different types of blur filters such as box blur, gaussian blur, and radial blur â€” each with its own advantages and trade-offs, and various strategies to implement them.

In this post, Iâ€™ll focus on the box blur. Weâ€™ll implement it in two waysâ€” a basic version without shared memory, and an optimized version that leverages the GPUâ€™s shared memory for better performance â€” and compare the results.

ðŸ‘‰ Box Blur
The idea is simple: for each pixel, given a radius, we read its neighbors, average their color values, and write the result. The larger the radius, the stronger the blur.

To apply the blur across the image, we need to launch one thread per pixel.
Remember, CUDA doesnâ€™t simply launch a flat number of threadsâ€”you define a grid of thread blocks and specify how many threads each block contains. 

In this example, we use a 2D ð™—ð™¡ð™¤ð™˜ð™ ð™Žð™žð™¯ð™š = 16Ã—16.
Then, we compute how many blocks (2D, again) are needed to cover the image:
ð™œð™§ð™žð™™ð™Žð™žð™¯ð™š = ð™žð™¢ð™–ð™œð™šð™Žð™žð™¯ð™š / ð™—ð™¡ð™¤ð™˜ð™ ð™Žð™žð™¯ð™š

Inside the kernel, each thread calculates its corresponding pixel position based on the block ID, block dimensions, and thread ID within the block and apply the blur logic.
Thatâ€™s all you need for a working blur.

ðŸŒŸ But hereâ€™s the catch: each thread reads RADIUS Ã— RADIUS pixels from global memory. This is slow and across thousands of threads, it adds up to significant memory bandwidth usage.
ðŸ”¥ And hereâ€™s the opportunity: neighboring threads within a block read overlapping regions.

Instead of having each thread fetch its own data from global memory, we load a tile of the image (block size + padding) into shared memory. All threads in the block cooperate to read this data, then synchronize, and apply the blur using the shared memory.
This significantly reduces global memory traffic and can result in up to a ~33% performance boost, depending on the hardware.

Thread block size and blur radius directly affect the performance.
Can you guess what would happen if we use 32Ã—32 blocks instead of 16Ã—16?

Source code -> GitHub.

ðŸ‘‰ Follow for more GPU insights!
#gpu #gpuprogramming #cuda #nvidia
