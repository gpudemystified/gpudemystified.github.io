#Linkedin

ğ™ğ™ğ™–ğ™§ğ™šğ™™ ğ™ˆğ™šğ™¢ğ™¤ğ™§ğ™®
Clearly, one of the most important concepts in GPU programming and architecture.

ğŸ§  The main idea?
Pretty straightforward: ğ™–ğ™˜ğ™˜ğ™šğ™¨ğ™¨ğ™ğ™£ğ™œ ğ™¢ğ™–ğ™ğ™£ ğ™¢ğ™šğ™¢ğ™¤ğ™§ğ™® ğ™ğ™¨ ğ™šğ™­ğ™©ğ™§ğ™šğ™¢ğ™šğ™¡ğ™® ğ™¨ğ™¡ğ™¤ğ™¬ compared to how fast GPU SIMD units can execute instructions â€” we're talking about an order of magnitude.
For example, the NVIDIA A100 can retrieve up to ğŸ® ğ—§ğ—•/ğ˜€ of data from memory to cores, while its cores can execute up to ğŸ­ğŸµ.ğŸ± ğ—§ğ—™ğ—Ÿğ—¢ğ—£ğ—¦ ğ—™ğ—£ğŸ¯ğŸ® or ğŸ¯ğŸ­ğŸ® ğ—§ğ—™ğ—Ÿğ—¢ğ—£ğ—¦ ğ—™ğ—£ğŸ­ğŸ²!

ğŸ‘‰ To overcome this bottleneck, GPU vendors designed a hierarchy of caches to exploit data reuse:
L2, L1, and most importantly, a ğ™¨ğ™¢ğ™–ğ™¡ğ™¡ (48â€“164 KB), ğ™«ğ™šğ™§ğ™® ğ™›ğ™–ğ™¨ğ™©, ğ™¤ğ™£-ğ™˜ğ™ğ™ğ™¥ memory inside the Streaming Multiprocessor (SM) â€” called ğ™ğ™ğ™–ğ™§ğ™šğ™™ ğ™ˆğ™šğ™¢ğ™¤ğ™§ğ™®. 
ğŸ—’ï¸ AMD calls it LDS (Local Data Share) - same concept, different name.

ğŸŒŸ Why is Shared Memory so important?
1ï¸âƒ£ Programmers are ğ™–ğ™¡ğ™¡ğ™¤ğ™¬ğ™šğ™™ ğ™©ğ™¤ ğ™ªğ™¨ğ™š ğ™ğ™© ğ™ğ™£ ğ™˜ğ™¤ğ™™ğ™š
You can explicitly tell the compiler: â€œAllocate this chunk in shared memory.â€ Just use the __shared__ keyword in front of the variable you want to place in shared memory.

This memory is: 
 â€¢ Statically allocated
 â€¢ The size is known at compile time
 â€¢ The size info is packed by the compiler into a metadata block that is sent to the GPU  along with the kernel
 â€¢ The SM uses this information to decide whether it can schedule a thread block based on its available shared memory

Once a thread block is scheduled on an SM, it occupies the required amount of shared memory until it finishes execution.

Example: 
__shared__ int s_mem[256];

2ï¸âƒ£  ğ™ğ™ğ™–ğ™§ğ™šğ™™ across the thread block
Every thread in a thread block has access to the same shared memory space.
This allows fast communication and synchronization between threads â€” something GPU programmers always want. -- that also means that all the threads part of a thread block have to execute on the same SM.

âš ï¸ Whether itâ€™s 32, 256, or 1024 threads â€” they all share the same block of shared memory.
ğŸ‘‰ Finding the right balance between threads per block and shared memory usage is crucial for performance.

3ï¸âƒ£ You must ğ™¢ğ™–ğ™£ğ™ªğ™–ğ™¡ğ™¡ğ™® ğ™¨ğ™®ğ™£ğ™˜ğ™ğ™§ğ™¤ğ™£ğ™ğ™¯ğ™š
Access must be explicitly synchronized. Warps might compete for shared memory access, so itâ€™s on the programmer to manage that. CUDA & HLSL provide __syncthreads() function for this purpose.

4ï¸âƒ£ ğ˜¼ğ™©ğ™¤ğ™¢ğ™ğ™˜ operations
Supports fast atomic operations (at a thread block level), leading to the concept of privatization.

5ï¸âƒ£ Huge ğ™¥ğ™šğ™§ğ™›ğ™¤ğ™§ğ™¢ğ™–ğ™£ğ™˜ğ™š benefits
Used correctly, it unlocks powerful optimizations:
 â€¢ Tiling techniques for image filtering or matrix multiplication
 â€¢ Histograms
 â€¢ Sorting algorithms

ğŸ”¥ Coming up: Weâ€™ll see how to practically use shared memory for the most common use cases.

ğŸ“± Donâ€™t forget that you can also find my posts on Instagram -> https://lnkd.in/dbKdgpE8

#GPU #GPUProgramming #GPUArchitecture #ParallelComputing #CUDA #NVIDIA #AMD #ComputerArchitecture #GPUDemystified


#Instagram
Clearly, one of the most important concepts in GPU programming and architecture.

ğŸ§  The main idea?
Pretty straightforward: accessing main memory is extremely slow compared to how fast GPU SIMD units can execute instructions.
For example, the NVIDIA A100 retrieves up to 2 TB/s of data from memory to cores, while its cores can execute up to 19.5 TFLOPS FP32 or 312 TFLOPS FP16!

ğŸ‘‰ To overcome this bottleneck, GPU vendors designed a hierarchy of caches:
L2, L1, and most importantly, a small (48â€“164 KB), very fast, on-chip memory inside the SM â€” called ğ™ğ™ğ™–ğ™§ğ™šğ™™ ğ™ˆğ™šğ™¢ğ™¤ğ™§ğ™®. 

ğŸŒŸ Why is Shared Memory so important?
1ï¸âƒ£ Programmers are allowed to use it in code
Just use the __shared__ keyword in front of the variable you want to place in shared memory.

This memory is: 
 â€¢ Statically allocated
 â€¢ The size is known at compile time
 â€¢ The size info is packed by the compiler into a metadata block that is sent to the GPU  along with the kernel
 â€¢ SM uses this information to decide whether it can schedule a thread block based on its available shared memory

Once a thread block is scheduled on an SM, it occupies the required amount of shared memory until it finishes execution.

2ï¸âƒ£ Shared across the thread block
Every thread in a thread block has access to the same shared memory space.
This allows fast communication and synchronization between threads. That also means that all the threads part of a thread block have to execute on the same SM.

âš ï¸ Whether itâ€™s 32, 256, or 1024 threads â€” they all share the same block of shared memory.
ğŸ‘‰ Finding the right balance between threads per block and shared memory usage is crucial for performance.

3ï¸âƒ£ You must manually syncrhonize
Access must be explicitly synchronized. Warps might compete for shared memory access, so itâ€™s on the programmer to manage that. CUDA & HLSL provide __syncthreads() function for this purpose.

4ï¸âƒ£ Atomic operations
Supports fast atomic operations (at a thread block level), leading to the concept of privatization.

5ï¸âƒ£ Huge performance benefits
Used correctly, it unlocks powerful optimizations:
 â€¢ Tiling
 â€¢ Histograms
 â€¢ Sorting

ğŸ‘‰ Follow for more GPU insights!
#GPU #GPUProgramming #CUDA #NVIDIA #AMD
